{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Random Seed: 1007 ===========\n"
     ]
    }
   ],
   "source": [
    "import h5py, torch, time, datetime, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn.modules import Module\n",
    "from tabulate import tabulate\n",
    "\n",
    "random_seed = torch.randint(0, 1339, (1,)).item()\n",
    "torch.manual_seed(random_seed)\n",
    "print('=========== Random Seed: %d ==========='%(random_seed))\n",
    "\n",
    "class DataFile():\n",
    "### Reads sample file Info (string), Parameters (list), Values (torch array), Data (torch array) and Weights (torch array)\n",
    "### FilePath is the path of the input file\n",
    "### Computes cross-section XS (average weight) and total number of data ND in file\n",
    "### Checks that files are in correct format (correct Keys)\n",
    "### and that the length of Parameters and Data equals the one of Values and Weights respectively\n",
    "    def __init__(self, FilePath, verbose=True):\n",
    "        if verbose: print('\\nReading file ...' + FilePath)\n",
    "        file = h5py.File(FilePath, 'r')\n",
    "        if list(file.keys()) == ['Data', 'Info', 'PDF_weights', 'Parameters', 'Process', 'Values', 'Weights']:\n",
    "            if( (len(file['Parameters'][()]) == len(file['Values'][()])) and (len(file['Data'][()]) == len(file['Weights'][()])) ):\n",
    "                if verbose: print('##### File Info:\\n' + file['Info'][()][0] + '\\n#####')\n",
    "                self.FilePath = FilePath\n",
    "                self.Info = file['Info'][()][0]\n",
    "                self.Process = file['Process'][()][0]\n",
    "                self.Parameters = file['Parameters'][()]\n",
    "                self.Values = torch.DoubleTensor(file['Values'][()])\n",
    "                self.Data = torch.DoubleTensor(file['Data'][()])\n",
    "                self.PDFWeights = torch.DoubleTensor(file['PDF_weights'][()])\n",
    "                self.Weights = torch.DoubleTensor(file['Weights'][()])\n",
    "                self.XS = self.Weights.mean()\n",
    "                self.ND = self.Data.size(0)\n",
    "            else: \n",
    "                print('--> File not valid:\\nunequal lenght of Values and Parameters or of Data and Weights')\n",
    "                raise ValueError\n",
    "        else:\n",
    "            print('--> File format not valid:\\nKeys: ' + str(list(file.keys())) + \n",
    "                  '\\nshould be: ' + str(['Data', 'Info', 'PDF_weights', 'Parameters', 'Process', 'Values', 'Weights']))\n",
    "            raise ValueError\n",
    "            \n",
    "            \n",
    "\n",
    "class OurTrainingData():\n",
    "### Imports data for training. The Return() methods returns [self.Data, self.Labels, self.Weights, self.ParVal]\n",
    "### All values are in double precision\n",
    "### Inputs are the SM and BSM file paths and list of integers to chop the datasets if needed\n",
    "### Weights are normalized to have sum = 1 on the entire training sample\n",
    "    def __init__(self, SMfilepathlist, BSMfilepathlist, process, parameters, SMNLimits=\"NA\", BSMNLimits=\"NA\", verbose=True): \n",
    "        self.Process = process\n",
    "        self.Parameters = parameters\n",
    "        self.BSMfilepathlist = BSMfilepathlist\n",
    "        self.SMfilepathlist = SMfilepathlist\n",
    "        if verbose: print('Loading Data Files for Process: ' + str(self.Process) +', with new physics Parameters: ' + str(self.Parameters) ) \n",
    "        #if len(self.Parameters)!= 1: print('Only 1D Implemented in Training !')   \n",
    "                \n",
    "####### Load BSM data (stored in self.BSMDataFiles)\n",
    "        if (not BSMfilepathlist):\n",
    "            # When studying nuisance parameters, BSM data can optional\n",
    "            print('No BSM files input. Skipping loading BSM files.')\n",
    "        elif type(BSMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in BSMfilepathlist):\n",
    "                self.BSMDataFiles = [] \n",
    "                for path in BSMfilepathlist:\n",
    "                    temp =  DataFile(path, verbose=verbose)\n",
    "                    if((temp.Process == self.Process) and (set(list(temp.Parameters.flatten())) == set(self.Parameters)) and (sum(temp.Values.flatten()) != 0.) ):\n",
    "                        self.BSMDataFiles.append(temp)\n",
    "                    else: \n",
    "                        print('File not valid: ' + path)\n",
    "                        print('Parameters = ' + str(temp.Parameters) + ', Process = ' + str(temp.Process) \n",
    "                              +' and Values = ' + str(temp.Values.tolist()))\n",
    "                        print('should be = ' + str(self.Parameters) + ', = ' + str(self.Process) \n",
    "                              + ' and != ' + str(0.))\n",
    "                        raise ValueError\n",
    "                        self.BSMDataFiles.append(None) \n",
    "            else:\n",
    "                print('BSMfilepathlist input should be a list of strings !')\n",
    "                raise FileNotFoundError\n",
    "        else:\n",
    "            print('BSMfilepathlist input should be a list !')\n",
    "            raise FileNotFoundError\n",
    "                  \n",
    "###### Chop the BSM data sets (stored in BSMNDList, BSMDataList, BSMWeightsList, BSMParValList, BSMTargetList)\n",
    "        if not BSMfilepathlist:\n",
    "            # When studying nuisance parameters, BSM data can optional.\n",
    "            # Here there is simply no BSM data to chop\n",
    "            print('No BSM files input. Skipping loading BSM data.')\n",
    "        else:\n",
    "            if type(BSMNLimits) == int:\n",
    "                BSMNLimits = [min(BSMNLimits, NF.ND) for NF in self.BSMDataFiles]\n",
    "            elif type(BSMNLimits) == list and all(isinstance(n, int) for n in BSMNLimits):\n",
    "                if len(BSMNLimits) != len(self.BSMDataFiles):\n",
    "                    print(\"--> Please input %d integers to chop each SM file.\"%(\n",
    "                        len(self.BSMDataFiles)))\n",
    "                    raise ValueError\n",
    "                elif sum([self.BSMDataFiles[i].ND >= BSMNLimits[i] for i in range(len(BSMNLimits))]\n",
    "                        ) != len(self.BSMDataFiles):\n",
    "                    print(\"--> Some chop limit larger than available data in the corresponding file.\")\n",
    "                    print(\"--> Lengths of the files: \"+str([file.ND for file in self.BSMDataFiles ]))\n",
    "                    raise ValueError\n",
    "            else:\n",
    "                BSMNLimits =[file.ND for file in self.BSMDataFiles]   \n",
    "\n",
    "            self.BSMNDList = BSMNLimits\n",
    "            #self.BSMNData = sum(self.BSMNDataList)\n",
    "            self.BSMDataList = [DF.Data[:N] for (DF, N) in zip(\n",
    "                self.BSMDataFiles, self.BSMNDList)]\n",
    "            self.BSMPDFWeightsList = [DF.PDFWeights[:N] for (DF, N) in zip(\n",
    "                self.BSMDataFiles, self.BSMNDList)] \n",
    "            self.BSMWeightsList = [DF.Weights[:N] for (DF, N) in zip(\n",
    "                self.BSMDataFiles, self.BSMNDList)] \n",
    "            self.BSMXSList = [DF.XS for DF in self.BSMDataFiles]\n",
    "            self.BSMParValList =  [torch.ones([N, len(self.Parameters)], dtype=torch.double)*DF.Values for (DF, N) in zip(self.BSMDataFiles, self.BSMNDList)]\n",
    "            self.BSMTargetList = [torch.ones(N, dtype=torch.double) for N in self.BSMNDList] \n",
    "\n",
    "\n",
    "####### Load SM data (stored in SMDataFiles)\n",
    "        if not SMfilepathlist:\n",
    "            # When studying nuisance parameters, SM data can be optional\n",
    "            print('No SM files input. Skipping loading SM files.')\n",
    "        elif type(SMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in SMfilepathlist):\n",
    "                #self.SMFilePathList = SMfilepathlist\n",
    "                #self.SMNumFiles = len(self.SMFilePathList)\n",
    "                self.SMDataFiles = []\n",
    "                for path in SMfilepathlist:\n",
    "                    temp =  DataFile(path, verbose=verbose)\n",
    "                    if( (temp.Process == self.Process) and (temp.Parameters[0] == 'SM') and (sum(temp.Values.flatten()) == 0.) ):\n",
    "                        self.SMDataFiles.append(temp)\n",
    "                    else:\n",
    "                        print('File not valid: ' + path)\n",
    "                        print('Parameters = ' + str(temp.Parameters) + ', Process = ' + str(temp.Process) \n",
    "                              +' and Values = ' + str(temp.Values.tolist()))\n",
    "                        print('should be = ' + 'SM'+ ', = ' + str(self.Process) \n",
    "                              + ' and = ' + str(0.))\n",
    "                        self.SMDataFiles.append(None)                    \n",
    "            else:\n",
    "                print('SMfilepathlist input should be a list of strings !')\n",
    "                raise FileNotFoundError\n",
    "        else:\n",
    "            print('SMfilepathlist input should be a list !')\n",
    "            raise FileNotFoundError\n",
    "            \n",
    "####### Chop the SM data sets and join them in one (stored in SMND, SMData and SMWeights)\n",
    "        if not SMfilepathlist:\n",
    "            # When studying nuisance parameters, BSM data can optional.\n",
    "            # Here there is simply no BSM data to chop\n",
    "            print('No SM files input. Skipping loading SM data.')\n",
    "        else:\n",
    "            if type(SMNLimits) == int:\n",
    "                SMNLimits = [min(SMNLimits, DF.ND) for DF in self.SMDataFiles]\n",
    "            elif type(SMNLimits) == list and all(isinstance(n, int) for n in SMNLimits):\n",
    "                if len(SMNLimits) != len(self.SMDataFiles):\n",
    "                    print(\"--> Please input %d integers to chop each SM file.\"%(\n",
    "                        len(self.SMDataFiles)))\n",
    "                    raise ValueError\n",
    "                elif sum([self.SMDataFiles[i].ND >= SMNLimits[i] for i in range(len(SMNLimits))]\n",
    "                        ) != len(self.SMDataFiles):\n",
    "                    print(\"--> Some chop limit larger than available data in the corresponding file.\")\n",
    "                    print(\"--> Lengths of the files: \" + str([file.ND for file in self.SMDataFiles]))\n",
    "                    raise ValueError\n",
    "            else:\n",
    "                SMNLimits = [file.ND for file in self.SMDataFiles]\n",
    "            self.SMND = sum(SMNLimits)\n",
    "            self.SMData = torch.cat(\n",
    "                [DF.Data[:N] for (DF, N) in zip(self.SMDataFiles, SMNLimits)]\n",
    "                , 0) \n",
    "            self.SMPDFWeights = torch.cat(\n",
    "                [DF.PDFWeights[:N] for (DF, N) in zip(self.SMDataFiles, SMNLimits)]\n",
    "                , 0)\n",
    "            self.SMWeights = torch.cat(\n",
    "                [DF.Weights[:N] for (DF, N) in zip(self.SMDataFiles, SMNLimits)]\n",
    "                , 0)\n",
    "            self.SMXSList = [DF.XS for DF in self.SMDataFiles]\n",
    "\n",
    "        if BSMfilepathlist and SMfilepathlist:\n",
    "            # only SM and BSM data\n",
    "            print(\"With SM and BSM files, breaking SM data blocks to be pared with BSM data.\")\n",
    "    ####### Break SM data in blocks to be paired with BSM data (stored in UsedSMNDList, UsedSMDataList, UsedSMWeightsList, UsedSMParValList, UsedSMTargetList)\n",
    "            BSMNRatioDataList = [torch.tensor(1., dtype=torch.double)*n/sum(self.BSMNDList\n",
    "                                                                           ) for n in self.BSMNDList]\n",
    "            self.UsedSMNDList = [int(self.SMND*BSMNRatioData) for BSMNRatioData in BSMNRatioDataList] \n",
    "            self.UsedSMDataList =  self.SMData[:sum(self.UsedSMNDList)].split(self.UsedSMNDList)\n",
    "            self.UsedSMPDFWeightsList = self.SMPDFWeights[:sum(self.UsedSMNDList)].split(self.UsedSMNDList)\n",
    "\n",
    "        ##### Reweighting is performed such that the SUM of the SM weights in each block equals the number of BSM data times the AVERAGE \n",
    "        ##### of the original weights. This equals the SM cross-section as obtained in the specific sample at hand, times NBSM\n",
    "            self.UsedSMWeightsList = self.SMWeights[:sum(self.UsedSMNDList)].split(self.UsedSMNDList)\n",
    "            self.UsedSMWeightsList = [ self.UsedSMWeightsList[i]*self.BSMNDList[i]/self.UsedSMNDList[i] for i in range(len(BSMNRatioDataList))]   \n",
    "            self.UsedSMParValList =  [torch.ones([N, len(self.Parameters)], dtype=torch.double)*DF.Values for (DF, N) in zip(self.BSMDataFiles, self.UsedSMNDList)]       \n",
    "            self.UsedSMTargetList = [torch.zeros(N, dtype=torch.double) for N in self.UsedSMNDList]\n",
    "\n",
    "    ####### Join SM with BSM data\n",
    "            self.Data = torch.cat(\n",
    "                [torch.cat([self.UsedSMDataList[i], self.BSMDataList[i]]\n",
    "                                      ) for i in range(len(self.BSMDataList))]\n",
    "                )\n",
    "            self.PDFWeights = torch.cat(\n",
    "                [torch.cat([self.UsedSMPDFWeightsList[i], self.BSMPDFWeightsList[i]]\n",
    "                                      ) for i in range(len(self.BSMPDFWeightsList))]\n",
    "                )\n",
    "            self.Weights = torch.cat(\n",
    "                [torch.cat([self.UsedSMWeightsList[i], self.BSMWeightsList[i]]\n",
    "                                      ) for i in range(len(self.BSMWeightsList))]\n",
    "                )\n",
    "            self.Labels = torch.cat(\n",
    "                [torch.cat([self.UsedSMTargetList[i], self.BSMTargetList[i]]\n",
    "                                      ) for i in range(len(self.BSMTargetList))]\n",
    "                )\n",
    "            self.ParVal = torch.cat(\n",
    "                [torch.cat([self.UsedSMParValList[i], self.BSMParValList[i]]\n",
    "                                      ) for i in range(len(self.BSMParValList))]\n",
    "                )\n",
    "        \n",
    "####### Simple assignment\n",
    "        if not BSMfilepathlist:\n",
    "            # only SM data\n",
    "            print('No BSM files input. Simply assign SM data.')\n",
    "            self.Data = self.SMData\n",
    "            self.PDFWeights = self.SMPDFWeights\n",
    "            self.Weights = self.SMWeights\n",
    "            self.Labels = torch.zeros(self.Data.size(0), dtype=torch.double)\n",
    "            self.ParVal = torch.zeros([self.Data.size(0), len(self.Parameters)], dtype=torch.double)\n",
    "        elif not SMfilepathlist:\n",
    "            # only BSM data\n",
    "            print('No SM files input. Simply assign BSM data.')\n",
    "            self.Data = torch.cat(self.BSMDataList)\n",
    "            self.PDFWeights = torch.cat(self.BSMPDFWeightsList)\n",
    "            self.Weights = torch.cat(self.BSMWeightsList)\n",
    "            self.Labels = torch.ones(self.Data.size(0), dtype=torch.double)\n",
    "            self.ParVal = torch.cat(self.BSMParValList)\n",
    "\n",
    "####### Final reweighting\n",
    "        s = self.Weights.sum()\n",
    "        self.Weights = self.Weights.div(s)\n",
    "\n",
    "####### If verbose, display report\n",
    "        if verbose: self.Report()\n",
    "        \n",
    "####### Return Tranining Data\n",
    "    def ReturnData(self):\n",
    "        return [self.Data, self.Labels, self.Weights, self.ParVal]\n",
    "                    \n",
    "    def Report(self):\n",
    "        #from tabulate import tabulate\n",
    "        if self.SMfilepathlist:\n",
    "            print('\\nLoaded SM Files:')\n",
    "            print(tabulate({str(self.Parameters): [ file.Values for file in self.SMDataFiles ], \n",
    "                            \"#Data\":[ file.ND for file in self.SMDataFiles ], \n",
    "                            \"XS[pb](avg.w)\":[ file.XS for file in self.SMDataFiles ]}, headers=\"keys\"))\n",
    "            \n",
    "        if self.BSMfilepathlist:\n",
    "            print('\\nLoaded BSM Files:')\n",
    "            print(tabulate({str(self.Parameters): [ file.Values for file in self.BSMDataFiles ], \n",
    "                            \"#Data\":[ file.ND for file in self.BSMDataFiles ], \n",
    "                            \"XS[pb](avg.w)\":[ file.XS for file in self.BSMDataFiles ]}, headers=\"keys\"))\n",
    "            \n",
    "        if self.SMfilepathlist and self.BSMfilepathlist:\n",
    "            print('\\nPaired BSM/SM Datasets:\\n')\n",
    "            ### Check should be nearly equal to #EV.BSM. It is computed with the weights BEFORE final reweighting\n",
    "            print(tabulate({str(self.Parameters): [ file.Values for file in self.BSMDataFiles ], \"#Ev.BSM\": self.BSMNDList\n",
    "                            , \"#Ev.SM\": self.UsedSMNDList,\n",
    "                            \"Check\": [(self.UsedSMWeightsList[i].sum())/(self.SMWeights.mean()) for i in range(len(self.BSMDataFiles))]\n",
    "                           }, headers=\"keys\"))    \n",
    "        \n",
    "####### Convert Angles\n",
    "    def CurateAngles(self, AnglePos):\n",
    "        Angles = self.Data[:, AnglePos]\n",
    "        CuratedAngles = torch.cat([torch.sin(Angles), torch.cos(Angles)], dim=1)\n",
    "        OtherPos = list(set(range(self.Data.size(1)))-set(AnglePos))\n",
    "        self.Data = torch.cat([self.Data[:, OtherPos], CuratedAngles], dim=1)\n",
    "        print('####\\nAnlges at position %s have been converted to Sin and Cos and put at the last columns of the Data.'%(AnglePos))\n",
    "        print('####')\n",
    "        \n",
    "        \n",
    "            \n",
    "####### Loss function(s), with \"input\" in (0,1) interval\n",
    "class _Loss(Module):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "            \n",
    "class WeightedSELoss(_Loss):\n",
    "    __constants__ = ['reduction']\n",
    "        \n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(WeightedSELoss, self).__init__(size_average, reduce, reduction)\n",
    "    def forward(self, input, target, weight):\n",
    "        return torch.sum(torch.mul(weight, (input - target)**2))\n",
    "\n",
    "class WeightedCELoss(_Loss):\n",
    "    __constants__ = ['reduction']\n",
    "        \n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(WeightedCELoss, self).__init__(size_average, reduce, reduction)\n",
    "    def forward(self, input, target, weight):\n",
    "        return torch.sum(torch.mul(weight, (1 - target)*torch.log(1./(1.-input))+target*torch.log(1./input)))\n",
    "    \n",
    "####### Loss function(s), with \"input\" in (0,1) interval\n",
    "def report_ETA(beginning, start, epochs, e, loss):\n",
    "    time_elapsed = time.time() - start\n",
    "    time_left    = str(datetime.timedelta(\n",
    "        seconds=((time.time() - beginning)/(e+1)*(epochs-(e+1)))))\n",
    "    print('Training epoch %s (took %.2f sec, time left %s sec) loss %.8f'%(\n",
    "        e, time_elapsed, time_left, loss))\n",
    "    return time.time()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class OurLinearModel(nn.Module):\n",
    "### Defines the  model with parametrized discriminant. Only quadratic dependence on a single parameter is implemented.\n",
    "### Input is the architecture (list of integers, the last one being equal to # of nuisance parameters) and the \n",
    "### activation type ('ReLU' or 'Sigmoid')\n",
    "    def __init__(self, NumberOfParameters, NumberOfNuisanceParameters, AR = [1, 3, 3, 1] , AF = 'ReLU'):               \n",
    "        super(OurLinearModel, self).__init__() \n",
    "        ValidActivationFunctions = {'ReLU': torch.relu, 'Sigmoid': torch.sigmoid}\n",
    "        try:\n",
    "            self.ActivationFunction = ValidActivationFunctions[AF]\n",
    "        except KeyError:\n",
    "            print('The activation function specified is not valid. Allowed activations are %s.'\n",
    "                 %str(list(ValidActivationFunctions.keys())))\n",
    "            print('Will use ReLU.')\n",
    "            self.ActivationFunction = torch.relu            \n",
    "        if type(AR) == list:\n",
    "            if( ( all(isinstance(n, int) for n in AR)) and ( AR[-1] == NumberOfNuisanceParameters) ):\n",
    "                self.Architecture = AR\n",
    "                self.NumberOfNuisanceParameters = NumberOfNuisanceParameters\n",
    "            else:\n",
    "                print('Architecture should be a list of integers, the last one should be %d.'%(NumberOfNuisanceParameters))\n",
    "                raise ValueError             \n",
    "        else:\n",
    "            print('Architecture should be a list !')\n",
    "            raise ValueError\n",
    "        self.DefineLayers(NumberOfParameters)\n",
    "\n",
    "### Define Layers. For the linear model, the number of nuisance networks is equal to the number of nuisance parameters\n",
    "    def DefineLayers(self, NumberOfParameters):\n",
    "        LinearLayers = [nn.Linear(self.Architecture[i], self.Architecture[i+1]) \\\n",
    "                                  for i in range(len(self.Architecture)-1)]\n",
    "        self.LinearLayers = nn.ModuleList(LinearLayers)        \n",
    "    \n",
    "### Forward Function. Performs Preprocessing, returns F = rho/(1+rho) in [0,1], where each component of rho corresponds\n",
    "### to the differential cross-section ratio under the nuisance parameter.\n",
    "    def Forward(self, Data, Parameters):\n",
    "        InputDimension = self.Architecture[0]\n",
    "        if Data.size(1) != InputDimension:\n",
    "            print('Dimensions of the data and the network input mismatch: data: %d, model: %d'\n",
    "                  %(Data.size(1), InputDimension))\n",
    "            raise ValueError\n",
    "\n",
    "        # Checking that preprocess has been initialised\n",
    "        if not hasattr(self, 'Shift'):\n",
    "            print('Please initialize preprocess parameters!')\n",
    "            raise ValueError\n",
    "        \n",
    "        # Parameter redundancy is not checked here because we could have been training the network on just SM or \n",
    "        # just BSM.\n",
    "            \n",
    "        with torch.no_grad(): \n",
    "            Data, Parameters = self.Preprocess(Data, Parameters)  \n",
    "         \n",
    "        x = Data\n",
    "        for Layer in self.LinearLayers[:-1]:\n",
    "            x = self.ActivationFunction(Layer(x))\n",
    "        # output layer ranges from -inf to +inf\n",
    "        x = self.LinearLayers[-1](x)\n",
    "        # linear parameterisation and yet positive definite\n",
    "        rho = (1+x)**2\n",
    "        \n",
    "        return (rho.div(1.+rho))\n",
    "    \n",
    "    def GetL1Bound(self, L1perUnit):\n",
    "        self.L1perUnit = L1perUnit\n",
    "    \n",
    "    def ClipL1Norm(self):\n",
    "### Clip the weights      \n",
    "        def ClipL1NormLayer(DesignatedL1Max, Layer, Counter):\n",
    "            if Counter == 1:\n",
    "                ### this avoids clipping the first layer\n",
    "                return\n",
    "            L1 = Layer.weight.abs().sum()\n",
    "            Layer.weight.masked_scatter_(L1 > DesignatedL1Max, \n",
    "                                        Layer.weight*(DesignatedL1Max/L1))\n",
    "            return\n",
    "        \n",
    "        Counter = 0\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                Counter += 1\n",
    "                with torch.no_grad():\n",
    "                    DesignatedL1Max = m.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                    ClipL1NormLayer(DesignatedL1Max, m, Counter)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    Counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        DesignatedL1Max = mm.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                        ClipL1NormLayer(DesignatedL1Max, mm, Counter)\n",
    "        return \n",
    "    \n",
    "    def DistributionRatio(self, points):\n",
    "### This is rho. I.e., after training, the estimator of the distribution ratio.\n",
    "        with torch.no_grad():\n",
    "            F = self(points)\n",
    "        return F/(1-F)\n",
    "    \n",
    "    def checkRedundancy(self, Parameters):\n",
    "### This is written specifically for 2D networks. It will check if any columns of the parameters are redundant \n",
    "### (i.e., full of zeros), and adjust the number of networks as well as the parameters scalings.\n",
    "### Of course the Forward function will also check the self.IsParamRedundant attribute to see which Parameters\n",
    "### to use.\n",
    "\n",
    "        print('====== Checking parameter redundancy. ======')\n",
    "        \n",
    "        Param_idx = torch.arange(Parameters.size(1))\n",
    "        zero_idx  = (torch.nonzero(Parameters[0] == Parameters[1]+Parameters[0])) # possible zero columns\n",
    "        zero_mask = torch.tensor([len(torch.nonzero(Parameters[:, zero_idx.squeeze()] !=0)\n",
    "                                     )!=0 if idx in zero_idx else True for idx in Param_idx])\n",
    "        self.IsParamRedundant = (sum(zero_mask) != Parameters.size(1))\n",
    "        print('====== IsParamRedundant: ' + str(self.IsParamRedundant))\n",
    "        if self.IsParamRedundant:\n",
    "            self.good_parameters = torch.nonzero(zero_mask)\n",
    "            print('====== Effective parameters: ' + str(list(self.good_parameters)))\n",
    "            self.DefineLayers(len(self.good_parameters))\n",
    "            \n",
    "\n",
    "    def InitPreprocess(self, Data, Parameters):\n",
    "### This can be run only ONCE to initialize the preprocess (shift and scaling) parameters\n",
    "### Takes as input the training Data and the training Parameters as Torch tensors.\n",
    "        \n",
    "        # Here we do not check redundancy\n",
    "        ### NOTICE: nuisance parameters do not enter the game until the loss function        \n",
    "        \n",
    "        if not hasattr(self, 'Scaling'):\n",
    "            print('Initializing Preprocesses Variables')\n",
    "            self.Scaling = Data.std(0)\n",
    "            self.Shift = Data.mean(0)\n",
    "            \n",
    "            self.ParameterScaling = Parameters.std(0)\n",
    "            # fill in zero scalings with ones\n",
    "            self.ParameterScaling[self.ParameterScaling == 0] = 1\n",
    "            \n",
    "        else: print('Preprocess can be initialized only once. Parameters unchanged.')\n",
    "            \n",
    "    def Preprocess(self, Data, Parameters):\n",
    "### Returns scaled/shifted data and parameters\n",
    "### Takes as input Data and Parameters as Torch tensors.\n",
    "        if  not hasattr(self, 'Scaling'): print('Preprocess parameters are not initialized.')\n",
    "        Data = (Data - self.Shift)/self.Scaling\n",
    "        Parameters = Parameters/self.ParameterScaling\n",
    "        return Data, Parameters\n",
    "    \n",
    "    def Save(self, Name, Folder, csvFormat=False):\n",
    "### Saves the model in Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        torch.save({'StateDict': self.state_dict(), \n",
    "                   'Scaling': self.Scaling,\n",
    "                   'Shift': self.Shift,\n",
    "                   'ParameterScaling': self.ParameterScaling}, \n",
    "                   FileName)\n",
    "        print('Model successfully saved.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "        if csvFormat:\n",
    "            modelparams = [w.detach().tolist() for w in self.parameters()]\n",
    "            np.savetxt(Folder + Name + ' (StateDict).csv', modelparams, '%s')\n",
    "            statistics = [self.Shift.detach().tolist(), self.Scaling.detach().tolist(),\n",
    "                         self.ParameterScaling.detach().tolist()]\n",
    "            np.savetxt(Folder + Name + ' (Statistics).csv', statistics, '%s')\n",
    "    \n",
    "    def Load(self, Name, Folder):\n",
    "### Loads the model from Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        try:\n",
    "            IncompatibleKeys = self.load_state_dict(torch.load(FileName)['StateDict'])\n",
    "        except KeyError:\n",
    "            print('No state dictionary saved. Loading model failed.')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[0]:\n",
    "            print('Missing Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[1]:\n",
    "            print('Unexpected Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        self.Scaling = torch.load(FileName)['Scaling']\n",
    "        self.Shift = torch.load(FileName)['Shift']\n",
    "        self.ParameterScaling = torch.load(FileName)['ParameterScaling']\n",
    "        \n",
    "        print('Model successfully loaded.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "    def Report(self): ### is it possibe to check if the model is in double?\n",
    "        print('\\nModel Report:')\n",
    "        print('Preprocess Initialized: ' + str(hasattr(self, 'Shift')))\n",
    "        print('Architecture: ' + str(self.Architecture))\n",
    "        print('Loss Function: ' + 'Quadratic')\n",
    "        print('Activation: ' + str(self.ActivationFunction))\n",
    "        \n",
    "    def cuda(self):\n",
    "        nn.Module.cuda(self)\n",
    "        self.Shift = self.Shift.cuda()\n",
    "        self.Scaling = self.Scaling.cuda()\n",
    "        self.ParameterScaling = self.ParameterScaling.cuda()\n",
    "        \n",
    "    def cpu(self):\n",
    "        self.Shift = self.Shift.cpu()\n",
    "        self.Scaling = self.Scaling.cpu()\n",
    "        self.ParameterScaling = self.ParameterScaling.cpu()\n",
    "        return nn.Module.cpu(self)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "import copy\n",
    "def OurCudaTensor(input):\n",
    "    output = copy.deepcopy(input)\n",
    "    output = output.cuda()\n",
    "    return output\n",
    "\n",
    "class OurTrainer(nn.Module):\n",
    "### Contains all parameters for training: Loss Function, Optimiser, NumberOfEpochs, InitialLearningRate, SaveAfterEpoch \n",
    "    def __init__(self, LearningRate = 1e-3, LossFunction = 'Quadratic', Optimiser = 'Adam', NumEpochs = 100):\n",
    "        super(OurTrainer, self).__init__() \n",
    "        self.NumberOfEpochs = NumEpochs\n",
    "        self.SaveAfterEpoch = lambda :[self.NumberOfEpochs,]\n",
    "        self.InitialLearningRate = LearningRate\n",
    "        ValidCriteria = {'Quadratic': WeightedSELoss(), 'CE':WeightedCELoss(),\n",
    "                        'PDFQuadratic': WeightedReplicaSELoss()}\n",
    "        try:\n",
    "            self.Criterion = ValidCriteria[LossFunction]\n",
    "        except KeyError:\n",
    "            print('The loss function specified is not valid. Allowed losses are %s.'\n",
    "                 %str(list(ValidCriteria)))\n",
    "            print('Will use Quadratic Loss.') \n",
    "        ValidOptimizers = {'Adam': torch.optim.Adam}\n",
    "        try:\n",
    "            self.Optimiser =  ValidOptimizers[Optimiser]\n",
    "        except KeyError:\n",
    "            print('The specified optimiser is not valid. Allowed optimisers are %s.'\n",
    "                 %str(list(ValidOptimisers)))\n",
    "            print('Will use Adam.')          \n",
    "    \n",
    "    def EstimateRequiredGPUMemory(self, model, Data, Parameters):\n",
    "        if next(model.parameters()).is_cuda:\n",
    "            print('Model is on cuda. No estimate possible anymore.')\n",
    "            return None\n",
    "        else:\n",
    "            before = torch.cuda.memory_allocated()\n",
    "            print(before)\n",
    "            ### Always make deep copy of objects before sending them to cuda. Delete when done\n",
    "            ModelCuda = copy.deepcopy(model)\n",
    "            ModelCuda.cuda()\n",
    "            DataCuda = OurCudaTensor(Data[:10000])\n",
    "            ParametersCuda = OurCudaTensor(Parameters[:10000])\n",
    "            print(torch.cuda.memory_allocated())\n",
    "            MF = ModelCuda.Forward(DataCuda, ParametersCuda)\n",
    "            after = torch.cuda.memory_allocated()\n",
    "            print(after)\n",
    "            del ModelCuda, DataCuda, ParametersCuda, MF\n",
    "            torch.cuda.empty_cache()        \n",
    "            estimate = float(Data.size()[0])/1e4*float(after-before)*1e-9\n",
    "            print(str(estimate) + ' GB')\n",
    "            return estimate\n",
    "        \n",
    "    def Train(self, model, Data, Parameters, Labels, Weights, bs = 100000, L1perUnit=None, UseGPU=True, Name=\"\", Folder=os.getcwd(), WeightClipping=False, L1Max=1):\n",
    "        \n",
    "        tempmodel = copy.deepcopy(model)\n",
    "        tempmodel.cuda()\n",
    "        tempData = OurCudaTensor(Data)\n",
    "        tempParameters = OurCudaTensor(Parameters)\n",
    "        tempLabels = OurCudaTensor(Labels)\n",
    "        tempWeights = OurCudaTensor(Weights)\n",
    "        \n",
    "        Optimiser = self.Optimiser(tempmodel.parameters(), self.InitialLearningRate)\n",
    "        mini_batch_size = bs\n",
    "        beginning = start = time.time()\n",
    "        \n",
    "        if WeightClipping:\n",
    "            tempmodel.GetL1Bound(L1Max)\n",
    "        \n",
    "        for e in range(self.NumberOfEpochs):\n",
    "            total_loss  = 0\n",
    "            #print(\"epoch\")\n",
    "            Optimiser.zero_grad()\n",
    "            for b in range(0, Data.size(0), mini_batch_size):\n",
    "                torch.cuda.empty_cache()\n",
    "                output          = tempmodel.Forward(tempData[b:b+mini_batch_size], tempParameters[b:b+mini_batch_size])\n",
    "                loss            = self.Criterion(output, tempLabels[b:b+mini_batch_size].reshape(-1,1), \n",
    "                                                 tempWeights[b:b+mini_batch_size].reshape(-1, 1), b, b+mini_batch_size)\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "            Optimiser.step()\n",
    "            \n",
    "            if WeightClipping:\n",
    "                tempmodel.ClipL1Norm()\n",
    "            \n",
    "            if (e+1) in self.SaveAfterEpoch():\n",
    "                start       = report_ETA(beginning, start, self.NumberOfEpochs, e+1, total_loss)\n",
    "                tempmodel.Save(Name + \"%d epoch\"%(e+1), Folder, csvFormat=True)\n",
    "        \n",
    "        tempmodel.Save(Name + 'Final', Folder, csvFormat=True)\n",
    "        \n",
    "        return tempmodel.cpu()\n",
    "    \n",
    "    def SetNumberOfEpochs(self, NE):\n",
    "        self.NumberOfEpochs = NE\n",
    "        \n",
    "    def SetInitialLearningRate(self,ILR):\n",
    "        self.InitialLearningRate = ILR\n",
    "        \n",
    "    def SetSaveAfterEpochs(self,SAE):\n",
    "        SAE.sort()\n",
    "        self.SaveAfterEpoch = lambda : SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedReplicaSELoss(_Loss):\n",
    "    __constants__ = ['reduction']\n",
    "        \n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(WeightedReplicaSELoss, self).__init__(size_average, reduce, reduction)\n",
    "        print('Please make sure that your PDF weights are properly normalised if you want to start with a loss near 0.25.')\n",
    "    def setparameters(self, pdfweight, totalpdfweight):\n",
    "        self.pdfweight, self.totalpdfweight = OurCudaTensor(pdfweight), OurCudaTensor(totalpdfweight)   \n",
    "    def forward(self, input, target, weight, b_start, b_end):\n",
    "        return torch.sum(((1./self.totalpdfweight)*(input**2) + (self.pdfweight[b_start:b_end])*((input-1)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Files for Process: W+Z, with new physics Parameters: ['Gphi[TeV**-2]', 'GW[TeV**-2]']\n",
      "No BSM files input. Skipping loading BSM files.\n",
      "No BSM files input. Skipping loading BSM data.\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Nuisance_Data/ChP_pt300_sm_nuisance1.h5\n",
      "##### File Info:\n",
      "{SM} = {0., 0.}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Nuisance_Events/ChP_pt300_sm_nuisance1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "No BSM files input. Simply assign SM data.\n",
      "\n",
      "Loaded SM Files:\n",
      "['Gphi[TeV**-2]', 'GW[TeV**-2]']           #Data    XS[pb](avg.w)\n",
      "---------------------------------------  -------  ---------------\n",
      "tensor([[0., 0.]], dtype=torch.float64)  3000000         0.734385\n",
      "####\n",
      "Anlges at position [3, 5] have been converted to Sin and Cos and put at the last columns of the Data.\n",
      "####\n",
      "Initializing Preprocesses Variables\n",
      "Please make sure that your PDF weights are properly normalised if you want to start with a loss near 0.25.\n",
      "Training epoch 10 (took 6.34 sec, time left 1:35:59.717686 sec) loss 0.25009558\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 10 epoch.pth\n",
      "Training epoch 100 (took 56.13 sec, time left 1:42:02.822718 sec) loss 0.25000209\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 100 epoch.pth\n",
      "Training epoch 500 (took 249.84 sec, time left 1:38:41.511853 sec) loss 0.24999841\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 500 epoch.pth\n",
      "Training epoch 1000 (took 313.62 sec, time left 1:33:47.162237 sec) loss 0.24999805\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 1000 epoch.pth\n",
      "Training epoch 2000 (took 626.20 sec, time left 1:23:25.422953 sec) loss 0.24999799\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 2000 epoch.pth\n",
      "Training epoch 3000 (took 620.81 sec, time left 1:12:48.147616 sec) loss 0.24999799\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 3000 epoch.pth\n",
      "Training epoch 4000 (took 622.97 sec, time left 1:02:22.329287 sec) loss 0.24999799\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 4000 epoch.pth\n",
      "Training epoch 5000 (took 621.67 sec, time left 0:51:56.352190 sec) loss 0.24999799\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 5000 epoch.pth\n",
      "Training epoch 6000 (took 619.80 sec, time left 0:41:30.562415 sec) loss 0.24999799\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 6000 epoch.pth\n",
      "Training epoch 7000 (took 619.78 sec, time left 0:31:06.475395 sec) loss 0.24999799\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 7000 epoch.pth\n",
      "Training epoch 8000 (took 620.18 sec, time left 0:20:43.564940 sec) loss 0.24999797\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 8000 epoch.pth\n",
      "Training epoch 9000 (took 620.11 sec, time left 0:10:21.251653 sec) loss 0.24999799\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 9000 epoch.pth\n",
      "Training epoch 10000 (took 620.02 sec, time left -1 day, 23:59:59.378311 sec) loss 0.24999797\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), 10000 epoch.pth\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChPsm, (Seed13), Final.pth\n",
      "Loading Data Files for Process: W-Z, with new physics Parameters: ['Gphi[TeV**-2]', 'GW[TeV**-2]']\n",
      "No BSM files input. Skipping loading BSM files.\n",
      "No BSM files input. Skipping loading BSM data.\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Nuisance_Data/ChM_pt300_sm_nuisance1.h5\n",
      "##### File Info:\n",
      "{SM} = {0., 0.}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Nuisance_Events/ChM_pt300_sm_nuisance1.dat.gz\n",
      "Charge = -1 --- Process = W-Z\n",
      "#####\n",
      "No BSM files input. Simply assign SM data.\n",
      "\n",
      "Loaded SM Files:\n",
      "['Gphi[TeV**-2]', 'GW[TeV**-2]']           #Data    XS[pb](avg.w)\n",
      "---------------------------------------  -------  ---------------\n",
      "tensor([[0., 0.]], dtype=torch.float64)  3000000         0.329268\n",
      "####\n",
      "Anlges at position [3, 5] have been converted to Sin and Cos and put at the last columns of the Data.\n",
      "####\n",
      "Initializing Preprocesses Variables\n",
      "Please make sure that your PDF weights are properly normalised if you want to start with a loss near 0.25.\n",
      "Training epoch 10 (took 6.24 sec, time left 1:34:24.914584 sec) loss 0.25029427\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChMsm, (Seed13), 10 epoch.pth\n",
      "Training epoch 100 (took 55.80 sec, time left 1:41:21.101754 sec) loss 0.25000367\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChMsm, (Seed13), 100 epoch.pth\n",
      "Training epoch 500 (took 247.99 sec, time left 1:37:58.446633 sec) loss 0.24999680\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChMsm, (Seed13), 500 epoch.pth\n",
      "Training epoch 1000 (took 310.00 sec, time left 1:32:54.210381 sec) loss 0.24999645\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChMsm, (Seed13), 1000 epoch.pth\n",
      "Training epoch 2000 (took 612.51 sec, time left 1:22:07.150642 sec) loss 0.24999636\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChMsm, (Seed13), 2000 epoch.pth\n",
      "Training epoch 3000 (took 609.48 sec, time left 1:11:36.045921 sec) loss 0.24999636\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/PDFLinearClassifier/TrainedModels/PDF_ChMsm, (Seed13), 3000 epoch.pth\n"
     ]
    }
   ],
   "source": [
    "DataFolder = '/data3/WZ_new_project/h5/Ideal_Nuisance_Data/'\n",
    "bs=100000\n",
    "\n",
    "td = OurTrainingData([DataFolder + 'ChP_pt300_sm_nuisance1.h5',],\n",
    "                     [],\n",
    "                     process = 'W+Z', parameters =['Gphi[TeV**-2]', 'GW[TeV**-2]'], \n",
    "                     SMNLimits=int(3e6),\n",
    "                     BSMNLimits=int(5e5))\n",
    "\n",
    "NumEpochs = int(1e4)\n",
    "#NumEpochs = 50\n",
    "\n",
    "td.Data = td.Data[:, :7]\n",
    "td.CurateAngles([3, 5])\n",
    "\n",
    "Data, ParVal, Labels, Weights, PDFWeights = td.Data, td.ParVal, td.Labels, td.Weights, td.PDFWeights\n",
    "Data, ParVal, Labels, Weights, PDFWeights = Data.float(), ParVal.float(), Labels.float(), Weights.float(), PDFWeights.float()\n",
    "\n",
    "#MD = OurLinearModel(NumberOfParameters=ParVal.size(1), NumberOfNuisanceParameters=PDFWeights.size(1), AR=[9,32,32,32,30])\n",
    "MD = OurLinearModel(NumberOfParameters=ParVal.size(1), NumberOfNuisanceParameters=PDFWeights.size(1), AR=[9,64,64,64,30])\n",
    "MD.InitPreprocess(Data, ParVal)\n",
    "\n",
    "OT = OurTrainer(NumEpochs=NumEpochs, LossFunction='PDFQuadratic')\n",
    "OT.SetSaveAfterEpochs([10,100,500]+list(range(1000, 11000, 1000)))\n",
    "#OT.SetSaveAfterEpochs([1, 2, 5]+list(range(10, 50, 10)))\n",
    "\n",
    "# PDFWeights read from data file is of the form wi/wc -1\n",
    "PDFWeights = PDFWeights+1\n",
    "totalPDFWeights = torch.sum(1+PDFWeights) # the extra 1 refers to central value which I define to be 1\n",
    "PDFWeights = PDFWeights/totalPDFWeights\n",
    "# feed PDFWeights into network\n",
    "OT.Criterion.setparameters(PDFWeights, totalPDFWeights)\n",
    "\n",
    "random_seed = torch.randint(0, 1339, (1,)).item()\n",
    "OT.Train(MD, Data = Data, Labels=Labels, Weights= Weights, Parameters=ParVal, bs=bs,\n",
    "        Name = 'PDF_ChPsm, (Seed%d), '%(random_seed), Folder = os.getcwd()+'/TrainedModels/')\n",
    "\n",
    "DataFolder = '/data3/WZ_new_project/h5/Ideal_Nuisance_Data/'\n",
    "\n",
    "td = OurTrainingData([DataFolder + 'ChM_pt300_sm_nuisance1.h5',],\n",
    "                     [],\n",
    "                     process = 'W-Z', parameters =['Gphi[TeV**-2]', 'GW[TeV**-2]'], \n",
    "                     SMNLimits=int(3e6),\n",
    "                     BSMNLimits=int(5e5))\n",
    "\n",
    "NumEpochs = int(1e4)\n",
    "#NumEpochs = 50\n",
    "\n",
    "td.Data = td.Data[:, :7]\n",
    "td.CurateAngles([3, 5])\n",
    "\n",
    "Data, ParVal, Labels, Weights, PDFWeights = td.Data, td.ParVal, td.Labels, td.Weights, td.PDFWeights\n",
    "Data, ParVal, Labels, Weights, PDFWeights = Data.float(), ParVal.float(), Labels.float(), Weights.float(), PDFWeights.float()\n",
    "\n",
    "MD = OurLinearModel(NumberOfParameters=ParVal.size(1), NumberOfNuisanceParameters=PDFWeights.size(1), AR=[9,64,64,64,30])\n",
    "#MD = OurLinearModel(NumberOfParameters=ParVal.size(1), NumberOfNuisanceParameters=PDFWeights.size(1), AR=[9,32,32,32,30])\n",
    "MD.InitPreprocess(Data, ParVal)\n",
    "\n",
    "OT = OurTrainer(NumEpochs=NumEpochs, LossFunction='PDFQuadratic')\n",
    "OT.SetSaveAfterEpochs([10,100,500]+list(range(1000, 11000, 1000)))\n",
    "#OT.SetSaveAfterEpochs([1, 2, 5]+list(range(10, 50, 10)))\n",
    "\n",
    "# PDFWeights read from data file is of the form wi/wc -1\n",
    "PDFWeights = PDFWeights+1\n",
    "totalPDFWeights = torch.sum(1+PDFWeights) # the extra 1 refers to central value which I define to be 1\n",
    "PDFWeights = PDFWeights/totalPDFWeights\n",
    "# feed PDFWeights into network\n",
    "OT.Criterion.setparameters(PDFWeights, totalPDFWeights)\n",
    "\n",
    "OT.Train(MD, Data = Data, Labels=Labels, Weights= Weights, Parameters=ParVal, bs=bs,\n",
    "        Name = 'PDF_ChMsm, (Seed%d), '%(random_seed), Folder = os.getcwd()+'/TrainedModels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFolder = '/data3/WZ_new_project/h5/Ideal_Nuisance_Data/'\n",
    "\n",
    "td = OurTrainingData([DataFolder + 'ChP_pt300_sm_nuisance1.h5',],\n",
    "                     [],\n",
    "                     process = 'W+Z', parameters =['Gphi[TeV**-2]', 'GW[TeV**-2]'], \n",
    "                     SMNLimits=int(3e6),\n",
    "                     BSMNLimits=int(5e5))\n",
    "\n",
    "NumEpochs = int(1e4)\n",
    "#NumEpochs = 50\n",
    "\n",
    "td.Data = td.Data[:, :7]\n",
    "td.CurateAngles([3, 5])\n",
    "\n",
    "Data, ParVal, Labels, Weights, PDFWeights = td.Data, td.ParVal, td.Labels, td.Weights, td.PDFWeights\n",
    "Data, ParVal, Labels, Weights, PDFWeights = Data.float(), ParVal.float(), Labels.float(), Weights.float(), PDFWeights.float()\n",
    "\n",
    "MD = OurLinearModel(NumberOfParameters=ParVal.size(1), NumberOfNuisanceParameters=PDFWeights.size(1), AR=[9,32,32,32,30])\n",
    "MD.InitPreprocess(Data, ParVal)\n",
    "\n",
    "OT = OurTrainer(NumEpochs=NumEpochs, LossFunction='PDFQuadratic')\n",
    "OT.SetSaveAfterEpochs([10,100,500]+list(range(1000, 11000, 1000)))\n",
    "#OT.SetSaveAfterEpochs([1, 2, 5]+list(range(10, 50, 10)))\n",
    "\n",
    "# PDFWeights read from data file is of the form wi/wc -1\n",
    "PDFWeights = PDFWeights+1\n",
    "totalPDFWeights = torch.sum(1+PDFWeights) # the extra 1 refers to central value which I define to be 1\n",
    "PDFWeights = PDFWeights/totalPDFWeights\n",
    "# feed PDFWeights into network\n",
    "OT.Criterion.setparameters(PDFWeights, totalPDFWeights)\n",
    "\n",
    "random_seed = torch.randint(0, 1339, (1,)).item()\n",
    "OT.Train(MD, Data = Data, Labels=Labels, Weights= Weights, Parameters=ParVal, bs=Data.size(0),\n",
    "        Name = 'PDF_ChPsm, (Seed%d), '%(random_seed), Folder = os.getcwd()+'/TrainedModels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFolder = '/data3/WZ_new_project/h5/Ideal_Nuisance_Data/'\n",
    "\n",
    "td = OurTrainingData([DataFolder + 'ChM_pt300_sm_nuisance1.h5',],\n",
    "                     [],\n",
    "                     process = 'W-Z', parameters =['Gphi[TeV**-2]', 'GW[TeV**-2]'], \n",
    "                     SMNLimits=int(3e6),\n",
    "                     BSMNLimits=int(5e5))\n",
    "\n",
    "NumEpochs = int(1e4)\n",
    "#NumEpochs = 50\n",
    "\n",
    "td.Data = td.Data[:, :7]\n",
    "td.CurateAngles([3, 5])\n",
    "\n",
    "Data, ParVal, Labels, Weights, PDFWeights = td.Data, td.ParVal, td.Labels, td.Weights, td.PDFWeights\n",
    "Data, ParVal, Labels, Weights, PDFWeights = Data.float(), ParVal.float(), Labels.float(), Weights.float(), PDFWeights.float()\n",
    "\n",
    "MD = OurLinearModel(NumberOfParameters=ParVal.size(1), NumberOfNuisanceParameters=PDFWeights.size(1), AR=[9,32,32,32,30])\n",
    "MD.InitPreprocess(Data, ParVal)\n",
    "\n",
    "OT = OurTrainer(NumEpochs=NumEpochs, LossFunction='PDFQuadratic')\n",
    "OT.SetSaveAfterEpochs([10,100,500]+list(range(1000, 11000, 1000)))\n",
    "#OT.SetSaveAfterEpochs([1, 2, 5]+list(range(10, 50, 10)))\n",
    "\n",
    "# PDFWeights read from data file is of the form wi/wc -1\n",
    "PDFWeights = PDFWeights+1\n",
    "totalPDFWeights = torch.sum(1+PDFWeights) # the extra 1 refers to central value which I define to be 1\n",
    "PDFWeights = PDFWeights/totalPDFWeights\n",
    "# feed PDFWeights into network\n",
    "OT.Criterion.setparameters(PDFWeights, totalPDFWeights)\n",
    "\n",
    "random_seed = torch.randint(0, 1339, (1,)).item()\n",
    "OT.Train(MD, Data = Data, Labels=Labels, Weights= Weights, Parameters=ParVal, bs=Data.size(0),\n",
    "        Name = 'PDF_ChMsm, (Seed%d), '%(random_seed), Folder = os.getcwd()+'/TrainedModels/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross check with Mathematica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFolder = '/data3/WZ_new_project/h5/Ideal_Nuisance_Data/'\n",
    "\n",
    "td = OurTrainingData([DataFolder + 'ChP_pt300_sm_nuisance1.h5',],\n",
    "                     [],\n",
    "                     process = 'W+Z', parameters =['Gphi[TeV**-2]', 'GW[TeV**-2]'], \n",
    "                     SMNLimits=int(3e6),\n",
    "                     BSMNLimits=int(5e5),\n",
    "                     verbose=False)\n",
    "\n",
    "NumEpochs = int(1e4)\n",
    "#NumEpochs = 50\n",
    "\n",
    "td.Data = td.Data[:, :7]\n",
    "td.CurateAngles([3, 5])\n",
    "\n",
    "Data, ParVal, Labels, Weights, PDFWeights = td.Data, td.ParVal, td.Labels, td.Weights, td.PDFWeights\n",
    "Data, ParVal, Labels, Weights, PDFWeights = Data.float(), ParVal.float(), Labels.float(), Weights.float(), PDFWeights.float()\n",
    "\n",
    "MD = OurLinearModel(NumberOfParameters=ParVal.size(1), NumberOfNuisanceParameters=PDFWeights.size(1), AR=[9,32,32,32,30])\n",
    "MD.InitPreprocess(Data, ParVal)\n",
    "\n",
    "MD.Load('PDF_ChPsm, (Seed112), 10000 epoch', os.getcwd()+'/TrainedModels/')\n",
    "MD.cpu()\n",
    "MD.Forward(Data[:1], ParVal[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.PDFWeights[0]+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.Weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.734385*3000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDFWeights read from data file is of the form wi/wc -1\n",
    "PDFWeights = PDFWeights+1\n",
    "totalPDFWeights = torch.sum(1+PDFWeights) # the extra 1 refers to central value which I define to be 1\n",
    "PDFWeights = PDFWeights/totalPDFWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDFWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MD.InitPreprocess(Data, ParVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = MD.Forward(Data, ParVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(PDFWeights+1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp * (PDFWeights+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(PDFWeights+1).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temppdfweights = PDFWeights+1\n",
    "totalweights = (temppdfweights+1).sum()\n",
    "\n",
    "tempcentralweights = torch.ones_like(temppdfweights)/totalweights\n",
    "temppdfweights = temppdfweights/totalweights\n",
    "\n",
    "(tempcentralweights+temppdfweights).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tempcentralweights*(temp**2) + (temppdfweights)*((temp-1)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(temppdfweights*0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
