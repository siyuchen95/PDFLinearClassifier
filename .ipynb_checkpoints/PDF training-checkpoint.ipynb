{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, torch, time, datetime, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn.modules import Module\n",
    "from tabulate import tabulate\n",
    "\n",
    "random_seed = torch.randint(0, 1339, (1,)).item()\n",
    "torch.manual_seed(random_seed)\n",
    "print('=========== Random Seed: %d ==========='%(random_seed))\n",
    "\n",
    "class DataFile():\n",
    "### Reads sample file Info (string), Parameters (list), Values (torch array), Data (torch array) and Weights (torch array)\n",
    "### FilePath is the path of the input file\n",
    "### Computes cross-section XS (average weight) and total number of data ND in file\n",
    "### Checks that files are in correct format (correct Keys)\n",
    "### and that the length of Parameters and Data equals the one of Values and Weights respectively\n",
    "    def __init__(self, FilePath, verbose=True):\n",
    "        if verbose: print('\\nReading file ...' + FilePath)\n",
    "        file = h5py.File(FilePath, 'r')\n",
    "        if list(file.keys()) == ['Data', 'Info', 'Parameters', 'Process', 'Values', 'Weights']:\n",
    "            if( (len(file['Parameters'][()]) == len(file['Values'][()])) and (len(file['Data'][()]) == len(file['Weights'][()])) ):\n",
    "                if verbose: print('##### File Info:\\n' + file['Info'][()][0] + '\\n#####')\n",
    "                self.FilePath = FilePath\n",
    "                self.Info = file['Info'][()][0]\n",
    "                self.Process = file['Process'][()][0]\n",
    "                self.Parameters = file['Parameters'][()]\n",
    "                #print(file['Values'][()].dtype)\n",
    "                #print('%.15f' % (file['Data'][()][0][0]))\n",
    "                self.Values = torch.DoubleTensor(file['Values'][()])\n",
    "                self.Data = torch.DoubleTensor(file['Data'][()])\n",
    "                self.Weights = torch.DoubleTensor(file['Weights'][()])\n",
    "                self.XS = self.Weights.mean()\n",
    "                self.ND = len(self.Weights)\n",
    "            else: \n",
    "                print('--> File not valid:\\nunequal lenght of Values and Parameters or of Data and Weights')\n",
    "                raise ValueError\n",
    "        else:\n",
    "            print('--> File format not valid:\\nKeys: ' + str(list(file.keys())) + \n",
    "                  '\\nshould be: ' + str(['Data', 'Info', 'Parameters', 'Process', 'Values', 'Weights']))\n",
    "            raise ValueError\n",
    "\n",
    "class OurTrainingData():\n",
    "### Imports data for training. The Return() methods returns [self.Data, self.Labels, self.Weights, self.ParVal]\n",
    "### All values are in double precision\n",
    "### Inputs are the SM and BSM file paths and list of integers to chop the datasets if needed\n",
    "### Weights are normalized to have sum = 1 on the entire training sample\n",
    "    def __init__(self, SMfilepathlist, BSMfilepathlist, process, parameters, SMNLimits=\"NA\", BSMNLimits=\"NA\", verbose=True): \n",
    "        self.Process = process\n",
    "        self.Parameters = parameters\n",
    "        if verbose: print('Loading Data Files for Process: ' + str(self.Process) +', with new physics Parameters: ' + str(self.Parameters) ) \n",
    "        #if len(self.Parameters)!= 1: print('Only 1D Implemented in Training !')   \n",
    "          \n",
    "####### Load BSM data (stored in self.BSMDataFiles)\n",
    "        if type(BSMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in BSMfilepathlist):\n",
    "                self.BSMDataFiles = [] \n",
    "                for path in BSMfilepathlist:\n",
    "                    temp =  DataFile(path, verbose=verbose)\n",
    "                    if((temp.Process == self.Process) and (set(list(temp.Parameters.flatten())) == set(self.Parameters)) and (sum(temp.Values.flatten()) != 0.) ):\n",
    "                        self.BSMDataFiles.append(temp)\n",
    "                    else: \n",
    "                        print('File not valid: ' + path)\n",
    "                        print('Parameters = ' + str(temp.Parameters) + ', Process = ' + str(temp.Process) \n",
    "                              +' and Values = ' + str(temp.Values.tolist()))\n",
    "                        print('should be = ' + str(self.Parameters) + ', = ' + str(self.Process) \n",
    "                              + ' and != ' + str(0.))\n",
    "                        raise ValueError\n",
    "                        self.BSMDataFiles.append(None) \n",
    "            else:\n",
    "                print('BSMfilepathlist input should be a list of strings !')\n",
    "                raise FileNotFoundError\n",
    "        else:\n",
    "            print('BSMfilepathlist input should be a list !')\n",
    "            raise FileNotFoundError\n",
    "                  \n",
    "###### Chop the BSM data sets (stored in BSMNDList, BSMDataList, BSMWeightsList, BSMParValList, BSMTargetList)\n",
    "        if type(BSMNLimits) == int:\n",
    "            BSMNLimits = [min(BSMNLimits, NF.ND) for NF in self.BSMDataFiles]\n",
    "        elif type(BSMNLimits) == list and all(isinstance(n, int) for n in BSMNLimits):\n",
    "            if len(BSMNLimits) != len(self.BSMDataFiles):\n",
    "                print(\"--> Please input %d integers to chop each SM file.\"%(\n",
    "                    len(self.BSMDataFiles)))\n",
    "                raise ValueError\n",
    "            elif sum([self.BSMDataFiles[i].ND >= BSMNLimits[i] for i in range(len(BSMNLimits))]\n",
    "                    ) != len(self.BSMDataFiles):\n",
    "                print(\"--> Some chop limit larger than available data in the corresponding file.\")\n",
    "                print(\"--> Lengths of the files: \"+str([file.ND for file in self.BSMDataFiles ]))\n",
    "                raise ValueError\n",
    "        else:\n",
    "            BSMNLimits =[file.ND for file in self.BSMDataFiles]   \n",
    "            \n",
    "        self.BSMNDList = BSMNLimits\n",
    "        #self.BSMNData = sum(self.BSMNDataList)\n",
    "        self.BSMDataList = [DF.Data[:N] for (DF, N) in zip(\n",
    "            self.BSMDataFiles, self.BSMNDList)]\n",
    "        self.BSMWeightsList = [DF.Weights[:N] for (DF, N) in zip(\n",
    "            self.BSMDataFiles, self.BSMNDList)] \n",
    "        self.BSMXSList = [DF.XS for DF in self.BSMDataFiles]\n",
    "        self.BSMParValList =  [torch.ones([N, len(self.Parameters)], dtype=torch.double)*DF.Values for (DF, N) in zip(self.BSMDataFiles, self.BSMNDList)]\n",
    "        self.BSMTargetList = [torch.ones(N, dtype=torch.double) for N in self.BSMNDList] \n",
    "        \n",
    "        \n",
    "####### Load SM data (stored in SMDataFiles)\n",
    "        if type(SMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in SMfilepathlist):\n",
    "                #self.SMFilePathList = SMfilepathlist\n",
    "                #self.SMNumFiles = len(self.SMFilePathList)\n",
    "                self.SMDataFiles = []\n",
    "                for path in SMfilepathlist:\n",
    "                    temp =  DataFile(path, verbose=verbose)\n",
    "                    if( (temp.Process == self.Process) and (temp.Parameters[0] == 'SM') and (sum(temp.Values.flatten()) == 0.) ):\n",
    "                        self.SMDataFiles.append(temp)\n",
    "                    else:\n",
    "                        print('File not valid: ' + path)\n",
    "                        print('Parameters = ' + str(temp.Parameters) + ', Process = ' + str(temp.Process) \n",
    "                              +' and Values = ' + str(temp.Values.tolist()))\n",
    "                        print('should be = ' + 'SM'+ ', = ' + str(self.Process) \n",
    "                              + ' and = ' + str(0.))\n",
    "                        self.SMDataFiles.append(None)                    \n",
    "            else:\n",
    "                print('SMfilepathlist input should be a list of strings !')\n",
    "                raise FileNotFoundError\n",
    "        else:\n",
    "            print('SMfilepathlist input should be a list !')\n",
    "            raise FileNotFoundError\n",
    "            \n",
    "####### Chop the SM data sets and join them in one (stored in SMND, SMData and SMWeights)\n",
    "        if type(SMNLimits) == int:\n",
    "            SMNLimits = [min(SMNLimits, DF.ND) for DF in self.SMDataFiles]\n",
    "        elif type(SMNLimits) == list and all(isinstance(n, int) for n in SMNLimits):\n",
    "            if len(SMNLimits) != len(self.SMDataFiles):\n",
    "                print(\"--> Please input %d integers to chop each SM file.\"%(\n",
    "                    len(self.SMDataFiles)))\n",
    "                raise ValueError\n",
    "            elif sum([self.SMDataFiles[i].ND >= SMNLimits[i] for i in range(len(SMNLimits))]\n",
    "                    ) != len(self.SMDataFiles):\n",
    "                print(\"--> Some chop limit larger than available data in the corresponding file.\")\n",
    "                print(\"--> Lengths of the files: \" + str([file.ND for file in self.SMDataFiles]))\n",
    "                raise ValueError\n",
    "        else:\n",
    "            SMNLimits = [file.ND for file in self.SMDataFiles]\n",
    "        self.SMND = sum(SMNLimits)\n",
    "        self.SMData = torch.cat(\n",
    "            [DF.Data[:N] for (DF, N) in zip(self.SMDataFiles, SMNLimits)]\n",
    "            , 0) \n",
    "        self.SMWeights = torch.cat(\n",
    "            [DF.Weights[:N] for (DF, N) in zip(self.SMDataFiles, SMNLimits)]\n",
    "            , 0)\n",
    "        self.SMXSList = [DF.XS for DF in self.SMDataFiles]\n",
    "        \n",
    "        \n",
    "        #idx_random = torch.randperm(self.SMND)\n",
    "        #self.SMData = self.SMData[idx_random, :]\n",
    "        #self.SMWeights = self.SMWeights[idx_random]\n",
    "\n",
    "####### Break SM data in blocks to be paired with BSM data (stored in UsedSMNDList, UsedSMDataList, UsedSMWeightsList, UsedSMParValList, UsedSMTargetList)\n",
    "        BSMNRatioDataList = [torch.tensor(1., dtype=torch.double)*n/sum(self.BSMNDList\n",
    "                                                                       ) for n in self.BSMNDList]\n",
    "        self.UsedSMNDList = [int(self.SMND*BSMNRatioData) for BSMNRatioData in BSMNRatioDataList] \n",
    "        #self.UsedSMNData = sum(self.UsedSMNDataList)\n",
    "        #self.UsedSMData = self.SMData[: self.UsedSMND]\n",
    "        self.UsedSMDataList =  self.SMData[:sum(self.UsedSMNDList)].split(self.UsedSMNDList)\n",
    "        \n",
    "    ##### Reweighting is performed such that the SUM of the SM weights in each block equals the number of BSM data times the AVERAGE \n",
    "    ##### of the original weights. This equals the SM cross-section as obtained in the specific sample at hand, times NBSM\n",
    "        self.UsedSMWeightsList = self.SMWeights[:sum(self.UsedSMNDList)].split(self.UsedSMNDList)\n",
    "        self.UsedSMWeightsList = [ self.UsedSMWeightsList[i]*self.BSMNDList[i]/self.UsedSMNDList[i] for i in range(len(BSMNRatioDataList))]   \n",
    "        self.UsedSMParValList =  [torch.ones([N, len(self.Parameters)], dtype=torch.double)*DF.Values for (DF, N) in zip(self.BSMDataFiles, self.UsedSMNDList)]       \n",
    "        self.UsedSMTargetList = [torch.zeros(N, dtype=torch.double) for N in self.UsedSMNDList]\n",
    "\n",
    "####### Join SM with BSM data\n",
    "        self.Data = torch.cat(\n",
    "            [torch.cat([self.UsedSMDataList[i], self.BSMDataList[i]]\n",
    "                                  ) for i in range(len(self.BSMDataList))]\n",
    "            )\n",
    "        self.Weights = torch.cat(\n",
    "            [torch.cat([self.UsedSMWeightsList[i], self.BSMWeightsList[i]]\n",
    "                                  ) for i in range(len(self.BSMWeightsList))]\n",
    "            )\n",
    "        self.Labels = torch.cat(\n",
    "            [torch.cat([self.UsedSMTargetList[i], self.BSMTargetList[i]]\n",
    "                                  ) for i in range(len(self.BSMTargetList))]\n",
    "            )\n",
    "        self.ParVal = torch.cat(\n",
    "            [torch.cat([self.UsedSMParValList[i], self.BSMParValList[i]]\n",
    "                                  ) for i in range(len(self.BSMParValList))]\n",
    "            )\n",
    "        \n",
    "####### Final reweighting\n",
    "        s = self.Weights.sum()\n",
    "        self.Weights = self.Weights.div(s)\n",
    "\n",
    "####### If verbose, display report\n",
    "        if verbose: self.Report()\n",
    "        \n",
    "####### Return Tranining Data\n",
    "    def ReturnData(self):\n",
    "        return [self.Data, self.Labels, self.Weights, self.ParVal]\n",
    "            \n",
    "    def Report(self):\n",
    "        #from tabulate import tabulate\n",
    "        print('\\nLoaded SM Files:')\n",
    "        print(tabulate({str(self.Parameters): [ file.Values for file in self.SMDataFiles ], \n",
    "                        \"#Data\":[ file.ND for file in self.SMDataFiles ], \n",
    "                        \"XS[pb](avg.w)\":[ file.XS for file in self.SMDataFiles ]}, headers=\"keys\"))\n",
    "        print('\\nLoaded BSM Files:')\n",
    "        print(tabulate({str(self.Parameters): [ file.Values for file in self.BSMDataFiles ], \n",
    "                        \"#Data\":[ file.ND for file in self.BSMDataFiles ], \n",
    "                        \"XS[pb](avg.w)\":[ file.XS for file in self.BSMDataFiles ]}, headers=\"keys\"))\n",
    "        print('\\nPaired BSM/SM Datasets:\\n')\n",
    "        ### Check should be nearly equal to #EV.BSM. It is computed with the weights BEFORE final reweighting\n",
    "        print(tabulate({str(self.Parameters): [ file.Values for file in self.BSMDataFiles ], \"#Ev.BSM\": self.BSMNDList\n",
    "                        , \"#Ev.SM\": self.UsedSMNDList,\n",
    "                        \"Check\": [(self.UsedSMWeightsList[i].sum())/(self.SMWeights.mean()) for i in range(len(self.BSMDataFiles))]\n",
    "                       }, headers=\"keys\"))    \n",
    "        \n",
    "####### Convert Angles\n",
    "    def CurateAngles(self, AnglePos):\n",
    "        Angles = self.Data[:, AnglePos]\n",
    "        CuratedAngles = torch.cat([torch.sin(Angles), torch.cos(Angles)], dim=1)\n",
    "        OtherPos = list(set(range(self.Data.size(1)))-set(AnglePos))\n",
    "        self.Data = torch.cat([self.Data[:, OtherPos], CuratedAngles], dim=1)\n",
    "        print('####\\nAnlges at position %s have been converted to Sin and Cos and put at the last columns of the Data.'%(AnglePos))\n",
    "        print('####')\n",
    "        \n",
    "        \n",
    "            \n",
    "####### Loss function(s), with \"input\" in (0,1) interval\n",
    "class _Loss(Module):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "            \n",
    "class WeightedSELoss(_Loss):\n",
    "    __constants__ = ['reduction']\n",
    "        \n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(WeightedSELoss, self).__init__(size_average, reduce, reduction)\n",
    "    def forward(self, input, target, weight):\n",
    "        return torch.sum(torch.mul(weight, (input - target)**2))\n",
    "\n",
    "class WeightedCELoss(_Loss):\n",
    "    __constants__ = ['reduction']\n",
    "        \n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(WeightedCELoss, self).__init__(size_average, reduce, reduction)\n",
    "    def forward(self, input, target, weight):\n",
    "        return torch.sum(torch.mul(weight, (1 - target)*torch.log(1./(1.-input))+target*torch.log(1./input)))\n",
    "    \n",
    "####### Loss function(s), with \"input\" in (0,1) interval\n",
    "def report_ETA(beginning, start, epochs, e, loss):\n",
    "    time_elapsed = time.time() - start\n",
    "    time_left    = str(datetime.timedelta(\n",
    "        seconds=((time.time() - beginning)/(e+1)*(epochs-(e+1)))))\n",
    "    print('Training epoch %s (took %.2f sec, time left %s sec) loss %.8f'%(\n",
    "        e, time_elapsed, time_left, loss))\n",
    "    return time.time()\n",
    "\n",
    "class OurModel(nn.Module):\n",
    "### Defines the  model with parametrized discriminant. Only quadratic dependence on a single parameter is implemented.\n",
    "### Input is the architecture (list of integers, the last one being equal to 1) and the activation type ('ReLU' or 'Sigmoid')\n",
    "    def __init__(self, AR = [1, 3, 3, 1] , AF = 'ReLU' ):               \n",
    "        super(OurModel, self).__init__() \n",
    "        ValidActivationFunctions = {'ReLU': torch.relu, 'Sigmoid': torch.sigmoid}\n",
    "        try:\n",
    "            self.ActivationFunction = ValidActivationFunctions[AF]\n",
    "        except KeyError:\n",
    "            print('The activation function specified is not valid. Allowed activations are %s.'\n",
    "                 %str(list(ValidActivationFunctions.keys())))\n",
    "            print('Will use ReLU.')\n",
    "            self.ActivationFunction = torch.relu            \n",
    "        if type(AR) == list:\n",
    "            if( ( all(isinstance(n, int) for n in AR)) and ( AR[-1] == 1) ):\n",
    "                self.Architecture = AR\n",
    "            else:\n",
    "                print('Architecture should be a list of integers, the last one should be 1.')\n",
    "                raise ValueError             \n",
    "        else:\n",
    "            print('Architecture should be a list !')\n",
    "            raise ValueError\n",
    "\n",
    "### Define Layers\n",
    "        #self.LinearLayerList1  = nn.ModuleList([nn.Linear(self.Architecture[i], \n",
    "        #    self.Architecture[i+1], bias=False) for i in range(len(self.Architecture)-2)])\n",
    "        self.LinearLayerList1  = nn.ModuleList([nn.Linear(self.Architecture[i], \n",
    "            self.Architecture[i+1]) for i in range(len(self.Architecture)-2)])\n",
    "        self.OutputLayer1 = nn.Linear(self.Architecture[-2], 1)       \n",
    "        #self.LinearLayerList2 = nn.ModuleList([nn.Linear(self.Architecture[i], \n",
    "        #    self.Architecture[i+1], bias=False) for i in range(len(self.Architecture)-2)])\n",
    "        self.LinearLayerList2 = nn.ModuleList([nn.Linear(self.Architecture[i], \n",
    "            self.Architecture[i+1]) for i in range(len(self.Architecture)-2)])\n",
    "        self.OutputLayer2 = nn.Linear(self.Architecture[-2], 1)\n",
    "        \n",
    "        #self.Optimiser = torch.optim.Adam(self.parameters(), self.InitialLearningRate)\n",
    "        #self.Criterion = WeightedMSELoss()\n",
    "\n",
    "    def Forward(self, Data, Parameters):\n",
    "### Forward Function. Performs Preprocessing, returns F = rho/(1+rho) in [0,1], where rho is quadratically parametrized.\n",
    "        # Checking that data has the right input dimension\n",
    "        InputDimension = self.Architecture[0]\n",
    "        if Data.size(1) != InputDimension:\n",
    "            print('Dimensions of the data and the network input mismatch: data: %d, model: %d'\n",
    "                  %(Data.size(1), InputDimension))\n",
    "            raise ValueError\n",
    "\n",
    "        # Checking that preprocess has been initialised\n",
    "        if not hasattr(self, 'Shift'):\n",
    "            print('Please initialize preprocess parameters!')\n",
    "            raise ValueError\n",
    "        with torch.no_grad(): \n",
    "            Data, Parameters = self.Preprocess(Data, Parameters)\n",
    "            \n",
    "        x1 = x2 = Data\n",
    "        \n",
    "        for i, Layer in enumerate(self.LinearLayerList1):\n",
    "            x1 = self.ActivationFunction(Layer(x1)) \n",
    "        x1 = self.OutputLayer1(x1).squeeze()\n",
    "        \n",
    "        for i, Layer in enumerate(self.LinearLayerList2):\n",
    "            x2 = self.ActivationFunction(Layer(x2))\n",
    "        #x2 = torch.exp(self.OutputLayer2(x2)).squeeze()\n",
    "        #x2 = torch.abs(self.OutputLayer2(x2)).squeeze()\n",
    "        x2 = self.OutputLayer2(x2).squeeze()\n",
    "        #x2 = self.OutputLayer2(x2).squeeze()\n",
    "        \n",
    "        \n",
    "        rho = (1 + torch.mul(x1, Parameters))**2 + (torch.mul(x2, Parameters))**2  \n",
    "        return (rho.div(1.+rho)).view(-1, 1)\n",
    "    \n",
    "    def GetL1Bound(self, L1perUnit):\n",
    "        self.L1perUnit = L1perUnit\n",
    "    \n",
    "    def ClipL1Norm(self):\n",
    "### Clip the weights      \n",
    "        def ClipL1NormLayer(DesignatedL1Max, Layer, Counter):\n",
    "            if Counter == 1:\n",
    "                ### this avoids clipping the first layer\n",
    "                return\n",
    "            L1 = Layer.weight.abs().sum()\n",
    "            Layer.weight.masked_scatter_(L1 > DesignatedL1Max, \n",
    "                                        Layer.weight*(DesignatedL1Max/L1))\n",
    "            return\n",
    "        \n",
    "        Counter = 0\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                Counter += 1\n",
    "                with torch.no_grad():\n",
    "                    DesignatedL1Max = m.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                    ClipL1NormLayer(DesignatedL1Max, m, Counter)\n",
    "            else:\n",
    "                for mm in m.children():\n",
    "                    Counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        DesignatedL1Max = mm.weight.size(0)*mm.weight.size(1)*self.L1perUnit\n",
    "                        ClipL1NormLayer(DesignatedL1Max, mm, Counter)\n",
    "        return \n",
    "    \n",
    "    def DistributionRatio(self, points):\n",
    "### This is rho. I.e., after training, the estimator of the distribution ratio.\n",
    "        with torch.no_grad():\n",
    "            F = self(points)\n",
    "        return F/(1-F)\n",
    "\n",
    "    def InitPreprocess(self, Data, Parameters):\n",
    "### This can be run only ONCE to initialize the preprocess (shift and scaling) parameters\n",
    "### Takes as input the training Data and the training Parameters as Torch tensors.\n",
    "        if not hasattr(self, 'Scaling'):\n",
    "            print('Initializing Preprocesses Variables')\n",
    "            self.Scaling = Data.std(0)\n",
    "            self.Shift = Data.mean(0)\n",
    "            self.ParameterScaling = Parameters.std(0)  \n",
    "        else: print('Preprocess can be initialized only once. Parameters unchanged.')\n",
    "            \n",
    "    def Preprocess(self, Data, Parameters):\n",
    "### Returns scaled/shifted data and parameters\n",
    "### Takes as input Data and Parameters as Torch tensors.\n",
    "        if  not hasattr(self, 'Scaling'): print('Preprocess parameters are not initialized.')\n",
    "        Data = (Data - self.Shift)/self.Scaling\n",
    "        Parameters = Parameters/self.ParameterScaling\n",
    "        return Data, Parameters\n",
    "    \n",
    "    def Save(self, Name, Folder, csvFormat=False):\n",
    "### Saves the model in Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        torch.save({'StateDict': self.state_dict(), \n",
    "                   'Scaling': self.Scaling,\n",
    "                   'Shift': self.Shift,\n",
    "                   'ParameterScaling': self.ParameterScaling}, \n",
    "                   FileName)\n",
    "        print('Model successfully saved.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "        if csvFormat:\n",
    "            modelparams = [w.detach().tolist() for w in self.parameters()]\n",
    "            np.savetxt(Folder + Name + ' (StateDict).csv', modelparams, '%s')\n",
    "            statistics = [self.Shift.detach().tolist(), self.Scaling.detach().tolist(),\n",
    "                         self.ParameterScaling.detach().tolist()]\n",
    "            np.savetxt(Folder + Name + ' (Statistics).csv', statistics, '%s')\n",
    "    \n",
    "    def Load(self, Name, Folder):\n",
    "### Loads the model from Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        try:\n",
    "            IncompatibleKeys = self.load_state_dict(torch.load(FileName)['StateDict'])\n",
    "        except KeyError:\n",
    "            print('No state dictionary saved. Loading model failed.')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[0]:\n",
    "            print('Missing Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[1]:\n",
    "            print('Unexpected Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        self.Scaling = torch.load(FileName)['Scaling']\n",
    "        self.Shift = torch.load(FileName)['Shift']\n",
    "        self.ParameterScaling = torch.load(FileName)['ParameterScaling']\n",
    "        \n",
    "        print('Model successfully loaded.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "    def Report(self): ### is it possibe to check if the model is in double?\n",
    "        print('\\nModel Report:')\n",
    "        print('Preprocess Initialized: ' + str(hasattr(self, 'Shift')))\n",
    "        print('Architecture: ' + str(self.Architecture))\n",
    "        print('Loss Function: ' + 'Quadratic')\n",
    "        print('Activation: ' + str(self.ActivationFunction))\n",
    "        \n",
    "    def cuda(self):\n",
    "        nn.Module.cuda(self)\n",
    "        self.Shift = self.Shift.cuda()\n",
    "        self.Scaling = self.Scaling.cuda()\n",
    "        self.ParameterScaling = self.ParameterScaling.cuda()\n",
    "        \n",
    "    def cpu(self):\n",
    "        self.Shift = self.Shift.cpu()\n",
    "        self.Scaling = self.Scaling.cpu()\n",
    "        self.ParameterScaling = self.ParameterScaling.cpu()\n",
    "        return nn.Module.cpu(self)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class OurCDModel(nn.Module):\n",
    "### Defines the  model with parametrized discriminant. Only quadratic dependence on a single parameter is implemented.\n",
    "### Input is the architecture (list of integers, the last one being equal to 1) and the activation type ('ReLU' or 'Sigmoid')\n",
    "    def __init__(self, NumberOfParameters, AR = [1, 3, 3, 1] , AF = 'ReLU'):               \n",
    "        super(OurCDModel, self).__init__() \n",
    "        ValidActivationFunctions = {'ReLU': torch.relu, 'Sigmoid': torch.sigmoid}\n",
    "        try:\n",
    "            self.ActivationFunction = ValidActivationFunctions[AF]\n",
    "        except KeyError:\n",
    "            print('The activation function specified is not valid. Allowed activations are %s.'\n",
    "                 %str(list(ValidActivationFunctions.keys())))\n",
    "            print('Will use ReLU.')\n",
    "            self.ActivationFunction = torch.relu            \n",
    "        if type(AR) == list:\n",
    "            if( ( all(isinstance(n, int) for n in AR)) and ( AR[-1] == 1) ):\n",
    "                self.Architecture = AR\n",
    "            else:\n",
    "                print('Architecture should be a list of integers, the last one should be 1.')\n",
    "                raise ValueError             \n",
    "        else:\n",
    "            print('Architecture should be a list !')\n",
    "            raise ValueError\n",
    "        self.DefineLayers(NumberOfParameters)\n",
    "\n",
    "### Define Layers\n",
    "    def DefineLayers(self, NumberOfParameters):\n",
    "        print('====== Defining layers for %d parameters. ======'%(NumberOfParameters))\n",
    "        self.NumberOfParameters = NumberOfParameters\n",
    "        self.NumberOfNetworks = int((2+self.NumberOfParameters)*(1+self.NumberOfParameters)/2)-1\n",
    "        LinearLayers = [([nn.Linear(self.Architecture[i], self.Architecture[i+1]) \\\n",
    "                                  for i in range(len(self.Architecture)-1)])\\\n",
    "                        for n in range(self.NumberOfNetworks)]\n",
    "        LinearLayers = [Layer for SubLayerList in LinearLayers for Layer in SubLayerList]\n",
    "        self.LinearLayers = nn.ModuleList(LinearLayers)\n",
    "    \n",
    "    def Forward(self, Data, Parameters):\n",
    "### Forward Function. Performs Preprocessing, returns F = rho/(1+rho) in [0,1], where rho is quadratically parametrized.\n",
    "        # Checking that data has the right input dimension\n",
    "        InputDimension = self.Architecture[0]\n",
    "        if Data.size(1) != InputDimension:\n",
    "            print('Dimensions of the data and the network input mismatch: data: %d, model: %d'\n",
    "                  %(Data.size(1), InputDimension))\n",
    "            raise ValueError\n",
    "\n",
    "        # Checking that preprocess has been initialised\n",
    "        if not hasattr(self, 'Shift'):\n",
    "            print('Please initialize preprocess parameters!')\n",
    "            raise ValueError\n",
    "        if not hasattr(self, 'IsParamRedundant'):\n",
    "            print('Please make sure that you have checked for Parameter redundancy.')\n",
    "            raise ValueError\n",
    "            \n",
    "        if self.IsParamRedundant:\n",
    "            #print('Parameter space is redundant.')\n",
    "            Parameters = Parameters[:, self.good_parameters]\n",
    "            \n",
    "        with torch.no_grad(): \n",
    "            Data, Parameters = self.Preprocess(Data, Parameters)  \n",
    "         \n",
    "        NumberOfLayers, NumberOfEvents = len(self.Architecture)-1, Data.size(0)\n",
    "        EntryIterator, NetworkIterator = 0, -1\n",
    "        MatrixLT = torch.zeros([NumberOfEvents, (self.NumberOfParameters+1)**2], dtype=Data.dtype)\n",
    "        \n",
    "        if Data.is_cuda:\n",
    "            MatrixLT = OurCudaTensor(MatrixLT)\n",
    "        \n",
    "        for i in range(self.NumberOfParameters+1):\n",
    "            EntryIterator += i\n",
    "            DiagonalEntry = True\n",
    "            for j in range(self.NumberOfParameters+1-i):\n",
    "                if NetworkIterator == -1:\n",
    "                    MatrixLT[:, EntryIterator] = torch.ones(NumberOfEvents)\n",
    "                    #print('Entry: %d, Layer: ones, DiagonalEntry: %s'%(EntryIterator,\n",
    "                    #                                                str(DiagonalEntry)))\n",
    "                else:\n",
    "                    x = Data\n",
    "                    for Layer in self.LinearLayers[NumberOfLayers*NetworkIterator:\\\n",
    "                                                  NumberOfLayers*(NetworkIterator+1)-1]:\n",
    "                        x = self.ActivationFunction(Layer(x))\n",
    "                    x = self.LinearLayers[NumberOfLayers*(NetworkIterator+1)-1](x).squeeze()\n",
    "                    #MatrixLT[:, EntryIterator] = torch.exp(x) if DiagonalEntry else x\n",
    "                    MatrixLT[:, EntryIterator] = x\n",
    "                    #print('Entry: %d, Layer: %d, DiagonalEntry: %s'%(EntryIterator, NetworkIterator, \n",
    "                    #                                                str(DiagonalEntry)))\n",
    "                EntryIterator += 1\n",
    "                NetworkIterator += 1\n",
    "                DiagonalEntry = False\n",
    "        #print('MatrixLT: '+str(MatrixLT.is_cuda))\n",
    "        #print('Parameters: '+str(Parameters.is_cuda))\n",
    "\n",
    "        MatrixLT = MatrixLT.reshape([-1, self.NumberOfParameters+1, self.NumberOfParameters+1]) \n",
    "        MatrixLTP = MatrixLT.matmul(Parameters.reshape([NumberOfEvents, self.NumberOfParameters+1, 1]))\n",
    "        rho = MatrixLTP.permute([0, 2, 1]).matmul(MatrixLTP).squeeze()\n",
    "        \n",
    "        return (rho.div(1.+rho)).view(-1, 1)\n",
    "    \n",
    "    def GetL1Bound(self, L1perUnit):\n",
    "        self.L1perUnit = L1perUnit\n",
    "    \n",
    "    def ClipL1Norm(self):\n",
    "### Clip the weights      \n",
    "        def ClipL1NormLayer(DesignatedL1Max, Layer, Counter):\n",
    "            if Counter == 1:\n",
    "                ### this avoids clipping the first layer\n",
    "                return\n",
    "            L1 = Layer.weight.abs().sum()\n",
    "            Layer.weight.masked_scatter_(L1 > DesignatedL1Max, \n",
    "                                        Layer.weight*(DesignatedL1Max/L1))\n",
    "            return\n",
    "        \n",
    "        Counter = 0\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                Counter += 1\n",
    "                with torch.no_grad():\n",
    "                    DesignatedL1Max = m.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                    ClipL1NormLayer(DesignatedL1Max, m, Counter)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    Counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        DesignatedL1Max = mm.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                        ClipL1NormLayer(DesignatedL1Max, mm, Counter)\n",
    "        return \n",
    "    \n",
    "    def DistributionRatio(self, points):\n",
    "### This is rho. I.e., after training, the estimator of the distribution ratio.\n",
    "        with torch.no_grad():\n",
    "            F = self(points)\n",
    "        return F/(1-F)\n",
    "    \n",
    "    def checkRedundancy(self, Parameters):\n",
    "### This is written specifically for 2D networks. It will check if any columns of the parameters are redundant \n",
    "### (i.e., full of zeros), and adjust the number of networks as well as the parameters scalings.\n",
    "### Of course the Forward function will also check the self.IsParamRedundant attribute to see which Parameters\n",
    "### to use.\n",
    "\n",
    "        print('====== Checking parameter redundancy. ======')\n",
    "        \n",
    "        Param_idx = torch.arange(Parameters.size(1))\n",
    "        zero_idx  = (torch.nonzero(Parameters[0] == Parameters[1]+Parameters[0])) # possible zero columns\n",
    "        zero_mask = torch.tensor([len(torch.nonzero(Parameters[:, zero_idx.squeeze()] !=0)\n",
    "                                     )!=0 if idx in zero_idx else True for idx in Param_idx])\n",
    "        self.IsParamRedundant = (sum(zero_mask) != Parameters.size(1))\n",
    "        print('====== IsParamRedundant: ' + str(self.IsParamRedundant))\n",
    "        if self.IsParamRedundant:\n",
    "            self.good_parameters = torch.nonzero(zero_mask)\n",
    "            print('====== Effective parameters: ' + str(list(self.good_parameters)))\n",
    "            self.DefineLayers(len(self.good_parameters))\n",
    "            \n",
    "\n",
    "    def InitPreprocess(self, Data, Parameters):\n",
    "### This can be run only ONCE to initialize the preprocess (shift and scaling) parameters\n",
    "### Takes as input the training Data and the training Parameters as Torch tensors.\n",
    "        \n",
    "        # check redunancy\n",
    "        self.checkRedundancy(Parameters)\n",
    "        \n",
    "        if not hasattr(self, 'Scaling'):\n",
    "            print('Initializing Preprocesses Variables')\n",
    "            self.Scaling = Data.std(0)\n",
    "            self.Shift = Data.mean(0)\n",
    "            if self.IsParamRedundant:\n",
    "                self.ParameterScaling = (Parameters[:, self.good_parameters]).std(0)\n",
    "                #print('Parameter scaling: '+str(self.ParameterScaling))\n",
    "            else:\n",
    "                self.ParameterScaling = Parameters.std(0)\n",
    "                #print('Parameter scaling: '+str(self.ParameterScaling))            \n",
    "        else: print('Preprocess can be initialized only once. Parameters unchanged.')\n",
    "            \n",
    "    def Preprocess(self, Data, Parameters):\n",
    "### Returns scaled/shifted data and parameters\n",
    "### Takes as input Data and Parameters as Torch tensors.\n",
    "        if  not hasattr(self, 'Scaling'): print('Preprocess parameters are not initialized.')\n",
    "        Data = (Data - self.Shift)/self.Scaling\n",
    "        Parameters = Parameters/self.ParameterScaling\n",
    "        Ones = torch.ones([Parameters.size(0),1], dtype=Parameters.dtype)\n",
    "        if Parameters.is_cuda:\n",
    "            Ones = Ones.cuda()\n",
    "        #print('Inside Preprocess, Data size: ')\n",
    "        #print(Data.size())\n",
    "        Parameters = torch.cat([Ones, Parameters.reshape(Data.size(0), -1)], dim=1)\n",
    "        return Data, Parameters\n",
    "    \n",
    "    def Save(self, Name, Folder, csvFormat=False):\n",
    "### Saves the model in Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        torch.save({'StateDict': self.state_dict(), \n",
    "                   'Scaling': self.Scaling,\n",
    "                   'Shift': self.Shift,\n",
    "                   'ParameterScaling': self.ParameterScaling}, \n",
    "                   FileName)\n",
    "        print('Model successfully saved.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "        if csvFormat:\n",
    "            modelparams = [w.detach().tolist() for w in self.parameters()]\n",
    "            np.savetxt(Folder + Name + ' (StateDict).csv', modelparams, '%s')\n",
    "            statistics = [self.Shift.detach().tolist(), self.Scaling.detach().tolist(),\n",
    "                         self.ParameterScaling.detach().tolist()]\n",
    "            np.savetxt(Folder + Name + ' (Statistics).csv', statistics, '%s')\n",
    "    \n",
    "    def Load(self, Name, Folder):\n",
    "### Loads the model from Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        try:\n",
    "            IncompatibleKeys = self.load_state_dict(torch.load(FileName)['StateDict'])\n",
    "        except KeyError:\n",
    "            print('No state dictionary saved. Loading model failed.')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[0]:\n",
    "            print('Missing Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[1]:\n",
    "            print('Unexpected Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        self.Scaling = torch.load(FileName)['Scaling']\n",
    "        self.Shift = torch.load(FileName)['Shift']\n",
    "        self.ParameterScaling = torch.load(FileName)['ParameterScaling']\n",
    "        \n",
    "        print('Model successfully loaded.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "    def Report(self): ### is it possibe to check if the model is in double?\n",
    "        print('\\nModel Report:')\n",
    "        print('Preprocess Initialized: ' + str(hasattr(self, 'Shift')))\n",
    "        print('Architecture: ' + str(self.Architecture))\n",
    "        print('Loss Function: ' + 'Quadratic')\n",
    "        print('Activation: ' + str(self.ActivationFunction))\n",
    "        \n",
    "    def cuda(self):\n",
    "        nn.Module.cuda(self)\n",
    "        self.Shift = self.Shift.cuda()\n",
    "        self.Scaling = self.Scaling.cuda()\n",
    "        self.ParameterScaling = self.ParameterScaling.cuda()\n",
    "        \n",
    "    def cpu(self):\n",
    "        self.Shift = self.Shift.cpu()\n",
    "        self.Scaling = self.Scaling.cpu()\n",
    "        self.ParameterScaling = self.ParameterScaling.cpu()\n",
    "        return nn.Module.cpu(self)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "import copy\n",
    "def OurCudaTensor(input):\n",
    "    output = copy.deepcopy(input)\n",
    "    output = output.cuda()\n",
    "    return output\n",
    "\n",
    "class OurTrainer(nn.Module):\n",
    "### Contains all parameters for training: Loss Function, Optimiser, NumberOfEpochs, InitialLearningRate, SaveAfterEpoch \n",
    "    def __init__(self, LearningRate = 1e-3, LossFunction = 'Quadratic', Optimiser = 'Adam', NumEpochs = 100):\n",
    "        super(OurTrainer, self).__init__() \n",
    "        self.NumberOfEpochs = NumEpochs\n",
    "        self.SaveAfterEpoch = lambda :[self.NumberOfEpochs,]\n",
    "        self.InitialLearningRate = LearningRate\n",
    "        ValidCriteria = {'Quadratic': WeightedSELoss(), 'CE':WeightedCELoss(), 'BCE':CrossEntropyLoss()}\n",
    "        try:\n",
    "            self.Criterion = ValidCriteria[LossFunction]\n",
    "        except KeyError:\n",
    "            print('The loss function specified is not valid. Allowed losses are %s.'\n",
    "                 %str(list(ValidCriteria)))\n",
    "            print('Will use Quadratic Loss.') \n",
    "        ValidOptimizers = {'Adam': torch.optim.Adam}\n",
    "        try:\n",
    "            self.Optimiser =  ValidOptimizers[Optimiser]\n",
    "        except KeyError:\n",
    "            print('The specified optimiser is not valid. Allowed optimisers are %s.'\n",
    "                 %str(list(ValidOptimisers)))\n",
    "            print('Will use Adam.')          \n",
    "    \n",
    "    def EstimateRequiredGPUMemory(self, model, Data, Parameters):\n",
    "        if next(model.parameters()).is_cuda:\n",
    "            print('Model is on cuda. No estimate possible anymore.')\n",
    "            return None\n",
    "        else:\n",
    "            before = torch.cuda.memory_allocated()\n",
    "            print(before)\n",
    "            ### Always make deep copy of objects before sending them to cuda. Delete when done\n",
    "            ModelCuda = copy.deepcopy(model)\n",
    "            ModelCuda.cuda()\n",
    "            DataCuda = OurCudaTensor(Data[:10000])\n",
    "            ParametersCuda = OurCudaTensor(Parameters[:10000])\n",
    "            print(torch.cuda.memory_allocated())\n",
    "            MF = ModelCuda.Forward(DataCuda, ParametersCuda)\n",
    "            after = torch.cuda.memory_allocated()\n",
    "            print(after)\n",
    "            del ModelCuda, DataCuda, ParametersCuda, MF\n",
    "            torch.cuda.empty_cache()        \n",
    "            estimate = float(Data.size()[0])/1e4*float(after-before)*1e-9\n",
    "            print(str(estimate) + ' GB')\n",
    "            return estimate\n",
    "        \n",
    "    def Train(self, model, Data, Parameters, Labels, Weights, bs = 100000, L1perUnit=None, UseGPU=True, Name=\"\", Folder=os.getcwd(), WeightClipping=False, L1Max=1):\n",
    "        \n",
    "        tempmodel = copy.deepcopy(model)\n",
    "        tempmodel.cuda()\n",
    "        tempData = OurCudaTensor(Data)\n",
    "        tempParameters = OurCudaTensor(Parameters)\n",
    "        tempLabels = OurCudaTensor(Labels)\n",
    "        tempWeights = OurCudaTensor(Weights)\n",
    "        \n",
    "        Optimiser = self.Optimiser(tempmodel.parameters(), self.InitialLearningRate)\n",
    "        mini_batch_size = bs\n",
    "        beginning = start = time.time()\n",
    "        \n",
    "        if WeightClipping:\n",
    "            tempmodel.GetL1Bound(L1Max)\n",
    "        \n",
    "        for e in range(self.NumberOfEpochs):\n",
    "            total_loss  = 0\n",
    "            #print(\"epoch\")\n",
    "            Optimiser.zero_grad()\n",
    "            for b in range(0, Data.size(0), mini_batch_size):\n",
    "                torch.cuda.empty_cache()\n",
    "                output          = tempmodel.Forward(tempData[b:b+mini_batch_size], tempParameters[b:b+mini_batch_size])\n",
    "                loss            = self.Criterion(output, tempLabels[b:b+mini_batch_size].reshape(-1,1), \n",
    "                                                 tempWeights[b:b+mini_batch_size].reshape(-1, 1))\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "            Optimiser.step()\n",
    "            \n",
    "            if WeightClipping:\n",
    "                tempmodel.ClipL1Norm()\n",
    "            \n",
    "            if (e+1) in self.SaveAfterEpoch():\n",
    "                start       = report_ETA(beginning, start, self.NumberOfEpochs, e+1, total_loss)\n",
    "                tempmodel.Save(Name + \"%d epoch\"%(e+1), Folder, csvFormat=True)\n",
    "        \n",
    "        tempmodel.Save(Name + 'Final', Folder, csvFormat=True)\n",
    "        \n",
    "        return tempmodel.cpu()\n",
    "    \n",
    "    def SetNumberOfEpochs(self, NE):\n",
    "        self.NumberOfEpochs = NE\n",
    "        \n",
    "    def SetInitialLearningRate(self,ILR):\n",
    "        self.InitialLearningRate = ILR\n",
    "        \n",
    "    def SetSaveAfterEpochs(self,SAE):\n",
    "        SAE.sort()\n",
    "        self.SaveAfterEpoch = lambda : SAE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
